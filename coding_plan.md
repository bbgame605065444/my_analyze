1. Project Goal
To replicate the core findings of the paper "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" by implementing a Python script that can perform both standard and Chain-of-Thought (CoT) prompting on a large language model and evaluate its performance on reasoning tasks.
2. Core Components
The script will be structured around the following key components:
Dataset Handling: Functions to load and preprocess datasets for various reasoning tasks (e.g., arithmetic, commonsense, symbolic).
Prompt Engineering: Functions to construct both standard and CoT prompts from the datasets.
Language Model Interaction: A function to send prompts to a language model API and retrieve the generated responses.
Response Parsing and Evaluation: Functions to parse the model's output, extract the final answer, and evaluate its correctness against the ground truth.
Experiment Orchestration: A main function to run the experiments, iterate through datasets and prompting methods, and report the results.
3. Function Definitions
Here is a breakdown of the functions to be implemented:
load_dataset(task_name)
Purpose: To load a specific dataset for a given reasoning task.
Parameters:
task_name (str): The name of the task (e.g., 'gsm8k', 'csqa', 'last_letter_concatenation').
Returns:
A list of dictionaries, where each dictionary represents a data point with keys like 'question', 'answer', and potentially 'chain_of_thought'.
Implementation Notes:
For this replication, we will use a small, representative sample of data for each task, hardcoded within the function. In a full-scale replication, this function would read from files (e.g., JSON, CSV).
create_few_shot_prompt(exemplars, new_question, use_cot)
Purpose: To construct the few-shot prompt to be sent to the language model.
Parameters:
exemplars (list): A list of dictionaries, each containing a 'question', 'answer', and 'chain_of_thought' for the few-shot examples.
new_question (str): The new question for which we want the model to generate an answer.
use_cot (bool): A flag to determine whether to use CoT prompting or standard prompting.
Returns:
A formatted string representing the complete prompt.
Implementation Notes:
The function will iterate through the exemplars and format them as "Q: [question]\nA: [chain_of_thought] [answer]" if use_cot is true, or "Q: [question]\nA: [answer]" if use_cot is false.
The new_question will be appended at the end, followed by "A:".
get_model_response(prompt)
Purpose: To interact with a large language model's API.
Parameters:
prompt (str): The prompt to be sent to the model.
Returns:
The model's generated text response as a string.
Implementation Notes:
This function will use the gemini-2.5-flash-preview-05-20 model.
It will include error handling and retry logic (exponential backoff) to manage potential API issues.
parse_response(response)
Purpose: To extract the final answer from the model's raw text response.
Parameters:
response (str): The text generated by the language model.
Returns:
The extracted answer as a string.
Implementation Notes:
The paper notes that the answer is typically the last numerical value or a simple "yes/no". This function will implement a simple heuristic to find the answer, such as looking for "The answer is" or extracting the last number.
evaluate_answer(predicted_answer, true_answer)
Purpose: To check if the predicted answer is correct.
Parameters:
predicted_answer (str): The answer extracted from the model's response.
true_answer (str): The ground truth answer from the dataset.
Returns:
True if the answers match, False otherwise.
Implementation Notes:
This function will perform string normalization (e.g., lowercasing, removing punctuation) to ensure a fair comparison.
run_experiment(task_name, use_cot)
Purpose: To orchestrate a single experiment for a given task and prompting method.
Parameters:
task_name (str): The name of the task.
use_cot (bool): Whether to use CoT prompting.
Returns:
The accuracy of the model on the task as a float.
Implementation Notes:
This function will:
Load the dataset using load_dataset.
Separate the dataset into few-shot exemplars and test questions.
Iterate through the test questions.
For each question, create a prompt using create_few_shot_prompt.
Get the model's response using get_model_response.
Parse the response to get the predicted answer using parse_response.
Evaluate the answer using evaluate_answer.
Calculate and return the overall accuracy.
4. Main Execution Block
The main part of the script will define the list of tasks to be evaluated.
It will then loop through each task and run two experiments: one with standard prompting (use_cot=False) and one with CoT prompting (use_cot=True).
Finally, it will print a summary of the results, comparing the performance of the two prompting methods for each task.
This plan provides a clear roadmap for developing the code. The next step is to implement these functions in Python.
