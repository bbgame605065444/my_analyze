"""
æ¨¡å‹æ¥å£ - Chain-of-Thought å®ç°
æ”¯æŒå¤šç§å¤§è¯­è¨€æ¨¡å‹ï¼šGemini APIã€Qwen3 8B (æœ¬åœ°/API)

CoTæ ¸å¿ƒæ¦‚å¿µï¼šè¿™ä¸ªæ¨¡å—è´Ÿè´£ä¸å¤§è¯­è¨€æ¨¡å‹é€šä¿¡
- æ¨¡å‹é€‰æ‹©ï¼šæ”¯æŒ Geminiã€Qwen3 8B æœ¬åœ°æ¨ç†ã€Qwen3 8B API
- APIè°ƒç”¨ï¼šå°†æ„å»ºå¥½çš„æç¤ºè¯å‘é€ç»™æ¨¡å‹
- é”™è¯¯å¤„ç†ï¼šç½‘ç»œé—®é¢˜æ—¶çš„é‡è¯•æœºåˆ¶
- FP8æ¨ç†ï¼šQwen3æ”¯æŒFP8é‡åŒ–ä»¥æå‡æ¨ç†é€Ÿåº¦
"""

import os
import time
import logging
from typing import Optional, Dict, Any

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ£€æŸ¥æ˜¯å¦å¯ç”¨ç¦»çº¿æ¨¡å¼
OFFLINE_MODE = os.getenv('OFFLINE_MODE', 'false').lower() == 'true'

if OFFLINE_MODE:
    logger.info("ğŸ”’ ç¦»çº¿æ¨¡å¼å·²å¯ç”¨ - ä»…ä½¿ç”¨Qwen3æœ¬åœ°æ¨ç†")
    # åœ¨ç¦»çº¿æ¨¡å¼ä¸‹ï¼Œç¦ç”¨æ‰€æœ‰åœ¨çº¿APIå¯¼å…¥
    GEMINI_AVAILABLE = False
    OPENAI_AVAILABLE = False
else:
    # æ­£å¸¸æ¨¡å¼ï¼šæ ¹æ®é…ç½®å¯¼å…¥ä¸åŒçš„ä¾èµ–
    try:
        import google.generativeai as genai
        GEMINI_AVAILABLE = True
        # é…ç½®Gemini APIå¯†é’¥
        if os.getenv('GEMINI_API_KEY'):
            genai.configure(api_key=os.getenv('GEMINI_API_KEY'))
    except ImportError:
        GEMINI_AVAILABLE = False
        logger.warning("Gemini API ä¸å¯ç”¨ï¼Œè¯·å®‰è£… google-generativeai")

    try:
        import openai
        OPENAI_AVAILABLE = True
    except ImportError:
        OPENAI_AVAILABLE = False
        logger.warning("OpenAI ä¸å¯ç”¨ï¼Œè¯·å®‰è£… openai")

# Qwen3æœ¬åœ°æ¨ç†ä¾èµ– (ç¦»çº¿æ¨¡å¼å¿…éœ€)
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer
    import torch
    TRANSFORMERS_AVAILABLE = True
    if OFFLINE_MODE:
        logger.info("âœ… Qwen3æœ¬åœ°æ¨ç†ä¾èµ–åŠ è½½æˆåŠŸ")
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    error_msg = "Transformers ä¸å¯ç”¨ï¼Œè¯·å®‰è£… transformers>=4.51.0 torch"
    if OFFLINE_MODE:
        error_msg += " (ç¦»çº¿æ¨¡å¼å¿…éœ€)"
    logger.error(error_msg)

# å…¨å±€å˜é‡å­˜å‚¨åŠ è½½çš„æ¨¡å‹
_local_model = None
_local_tokenizer = None

class ModelConfig:
    """æ¨¡å‹é…ç½®ç±»"""
    def __init__(self):
        # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼Œé»˜è®¤ä½¿ç”¨Gemini
        self.model_type = os.getenv('MODEL_TYPE', 'gemini')  # 'gemini', 'qwen3_local', 'qwen3_api'
        self.model_name = os.getenv('MODEL_NAME', 'gemini-2.0-flash-exp')
        self.api_key = os.getenv('API_KEY', os.getenv('GEMINI_API_KEY'))
        self.api_base = os.getenv('API_BASE', 'https://api.openai.com/v1')
        
        # Qwen3 ç‰¹å®šé…ç½®
        self.qwen_model_path = os.getenv('QWEN_MODEL_PATH', 'Qwen/Qwen3-8B')
        self.use_fp8 = os.getenv('USE_FP8', 'false').lower() == 'true'
        self.device_map = os.getenv('DEVICE_MAP', 'auto')
        self.torch_dtype = os.getenv('TORCH_DTYPE', 'auto')
        
        # ç”Ÿæˆå‚æ•° (éæ€è€ƒæ¨¡å¼)
        self.temperature = float(os.getenv('TEMPERATURE', '0.7'))
        self.top_p = float(os.getenv('TOP_P', '0.8'))
        self.max_tokens = int(os.getenv('MAX_TOKENS', '2048'))

def load_qwen3_local(config: ModelConfig):
    """åŠ è½½æœ¬åœ°Qwen3æ¨¡å‹"""
    global _local_model, _local_tokenizer
    
    if _local_model is None:
        logger.info(f"æ­£åœ¨åŠ è½½Qwen3æ¨¡å‹: {config.qwen_model_path}")
        
        # åŠ è½½tokenizer
        _local_tokenizer = AutoTokenizer.from_pretrained(
            config.qwen_model_path,
            trust_remote_code=True
        )
        
        # é…ç½®æ¨¡å‹åŠ è½½å‚æ•°
        model_kwargs = {
            'trust_remote_code': True,
            'device_map': config.device_map
        }
        
        # è®¾ç½®æ•°æ®ç±»å‹
        if config.torch_dtype == 'auto':
            model_kwargs['torch_dtype'] = 'auto'
        elif config.use_fp8:
            # FP8é‡åŒ–é…ç½®
            try:
                from transformers import BitsAndBytesConfig
                quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    llm_int8_enable_fp32_cpu_offload=True
                )
                model_kwargs['quantization_config'] = quantization_config
                logger.info("å¯ç”¨FP8é‡åŒ–")
            except ImportError:
                logger.warning("BitsAndBytesConfigä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤ç²¾åº¦")
                model_kwargs['torch_dtype'] = torch.float16
        else:
            model_kwargs['torch_dtype'] = torch.float16
        
        # åŠ è½½æ¨¡å‹
        _local_model = AutoModelForCausalLM.from_pretrained(
            config.qwen_model_path,
            **model_kwargs
        )
        
        logger.info("Qwen3æ¨¡å‹åŠ è½½å®Œæˆ")
    
    return _local_model, _local_tokenizer

def get_model_response(prompt: str, config: Optional[ModelConfig] = None, max_retries: int = 3, base_delay: int = 1) -> str:
    """
    å‘è¯­è¨€æ¨¡å‹å‘é€æç¤ºå¹¶è·å–å“åº”ã€‚
    
    å‚æ•°è¯´æ˜ï¼š
        prompt (str): è¦å‘é€ç»™æ¨¡å‹çš„æç¤ºè¯
        config (ModelConfig): æ¨¡å‹é…ç½®ï¼Œé»˜è®¤ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®
        max_retries (int): æœ€å¤§é‡è¯•æ¬¡æ•°
        base_delay (int): æŒ‡æ•°é€€é¿çš„åŸºç¡€å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        
    è¿”å›å€¼ï¼š
        str: æ¨¡å‹çš„å“åº”æ–‡æœ¬
        
    å¼‚å¸¸ï¼š
        Exception: å¦‚æœæ‰€æœ‰é‡è¯•å°è¯•éƒ½å¤±è´¥
        
    CoTå­¦ä¹ è¦ç‚¹ï¼š
    - è¿™ä¸ªå‡½æ•°æ¥æ”¶å®Œæ•´çš„æç¤ºè¯ï¼ˆåŒ…å«few-shotç¤ºä¾‹å’Œæ–°é—®é¢˜ï¼‰
    - æ¨¡å‹æ ¹æ®æç¤ºè¯ä¸­çš„ç¤ºä¾‹å­¦ä¹ å›ç­”æ ¼å¼
    - å¦‚æœæ˜¯CoTæç¤ºï¼Œæ¨¡å‹ä¼šæ¨¡ä»¿ç¤ºä¾‹ä¸­çš„é€æ­¥æ¨ç†é£æ ¼
    - ç¦»çº¿æ¨¡å¼ä¸‹ä»…æ”¯æŒQwen3æœ¬åœ°æ¨ç†
    """
    
    if config is None:
        config = ModelConfig()
    
    # ç¦»çº¿æ¨¡å¼éªŒè¯
    if OFFLINE_MODE:
        if config.model_type != 'qwen3_local':
            logger.warning(f"ğŸ”’ ç¦»çº¿æ¨¡å¼ä¸‹å¼ºåˆ¶ä½¿ç”¨Qwen3æœ¬åœ°æ¨ç†ï¼Œå¿½ç•¥é…ç½®çš„æ¨¡å‹ç±»å‹: {config.model_type}")
            config.model_type = 'qwen3_local'
        
        # ç¡®ä¿ç¦»çº¿æ¨¡å¼ä¸‹ä¸ä½¿ç”¨åœ¨çº¿API
        if config.model_type in ['gemini', 'qwen3_api']:
            raise ValueError(f"ğŸš« ç¦»çº¿æ¨¡å¼ä¸‹ä¸å…è®¸ä½¿ç”¨åœ¨çº¿API: {config.model_type}")
    
    # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒçš„æ¨ç†æ–¹å¼
    if config.model_type == 'gemini':
        if OFFLINE_MODE:
            raise ValueError("ğŸš« ç¦»çº¿æ¨¡å¼ä¸‹ç¦ç”¨Gemini API")
        return _get_gemini_response(prompt, config, max_retries, base_delay)
    elif config.model_type == 'qwen3_local':
        return _get_qwen3_local_response(prompt, config, max_retries, base_delay)
    elif config.model_type == 'qwen3_api':
        if OFFLINE_MODE:
            raise ValueError("ğŸš« ç¦»çº¿æ¨¡å¼ä¸‹ç¦ç”¨Qwen3 API")
        return _get_qwen3_api_response(prompt, config, max_retries, base_delay)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {config.model_type}")

def _get_gemini_response(prompt: str, config: ModelConfig, max_retries: int, base_delay: int) -> str:
    """Gemini APIå“åº”"""
    if OFFLINE_MODE:
        raise ValueError("ğŸš« ç¦»çº¿æ¨¡å¼ä¸‹ç¦ç”¨Gemini API")
    
    if not GEMINI_AVAILABLE:
        raise ImportError("Gemini API ä¸å¯ç”¨ï¼Œè¯·å®‰è£… google-generativeai")
    
    # åˆ›å»ºGeminiæ¨¡å‹å®ä¾‹
    model = genai.GenerativeModel(config.model_name)
    
    # é‡è¯•å¾ªç¯ï¼šå¤„ç†ç½‘ç»œé”™è¯¯å’ŒAPIé™åˆ¶
    for attempt in range(max_retries + 1):
        try:
            # å‘æ¨¡å‹å‘é€æç¤ºè¯å¹¶è·å–å“åº”
            response = model.generate_content(prompt)
            return response.text
            
        except Exception as e:
            if attempt == max_retries:
                raise e
            
            delay = base_delay * (2 ** attempt)
            logger.warning(f"Gemini APIè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries + 1}). {delay}ç§’åé‡è¯•...")
            time.sleep(delay)
    
    raise Exception("è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°")

def _get_qwen3_local_response(prompt: str, config: ModelConfig, max_retries: int, base_delay: int) -> str:
    """Qwen3æœ¬åœ°æ¨ç†å“åº”"""
    if not TRANSFORMERS_AVAILABLE:
        raise ImportError("Transformers ä¸å¯ç”¨ï¼Œè¯·å®‰è£… transformers>=4.51.0 torch")
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_qwen3_local(config)
    
    for attempt in range(max_retries + 1):
        try:
            # æ„å»ºæ¶ˆæ¯æ ¼å¼
            messages = [{"role": "user", "content": prompt}]
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # ç¼–ç è¾“å…¥
            model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
            
            # ç”Ÿæˆå“åº” (éæ€è€ƒæ¨¡å¼å‚æ•°)
            with torch.no_grad():
                generated_ids = model.generate(
                    **model_inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            # è§£ç å“åº”
            generated_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            return response.strip()
            
        except Exception as e:
            if attempt == max_retries:
                raise e
            
            delay = base_delay * (2 ** attempt)
            logger.warning(f"Qwen3æœ¬åœ°æ¨ç†å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries + 1}). {delay}ç§’åé‡è¯•...")
            time.sleep(delay)
    
    raise Exception("è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°")

def _get_qwen3_api_response(prompt: str, config: ModelConfig, max_retries: int, base_delay: int) -> str:
    """Qwen3 APIå“åº”"""
    if OFFLINE_MODE:
        raise ValueError("ğŸš« ç¦»çº¿æ¨¡å¼ä¸‹ç¦ç”¨Qwen3 API")
    
    if not OPENAI_AVAILABLE:
        raise ImportError("OpenAIåº“ä¸å¯ç”¨ï¼Œè¯·å®‰è£… openai")
    
    # é…ç½®APIå®¢æˆ·ç«¯
    client = openai.OpenAI(
        api_key=config.api_key,
        base_url=config.api_base
    )
    
    for attempt in range(max_retries + 1):
        try:
            # è°ƒç”¨API
            response = client.chat.completions.create(
                model=config.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=config.temperature,
                top_p=config.top_p,
                max_tokens=config.max_tokens
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            if attempt == max_retries:
                raise e
            
            delay = base_delay * (2 ** attempt)
            logger.warning(f"Qwen3 APIè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries + 1}). {delay}ç§’åé‡è¯•...")
            time.sleep(delay)
    
    raise Exception("è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°")