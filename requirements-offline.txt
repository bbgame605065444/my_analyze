# CoT 离线模式依赖库 - 仅Qwen3本地推理
# 此文件仅包含Qwen3本地推理必需的依赖，无任何在线API依赖

# =============================================================================
# 核心依赖 (必需)
# =============================================================================

# 基础数值计算
numpy>=1.21.0

# =============================================================================
# Qwen3 本地推理依赖 (必需)
# =============================================================================

# PyTorch - 深度学习框架
torch>=2.0.0

# Transformers - HuggingFace模型库
transformers>=4.51.0

# Accelerate - 模型加速库
accelerate>=0.20.0

# =============================================================================
# 性能优化依赖 (可选但推荐)
# =============================================================================

# BitsAndBytes - FP8量化支持
bitsandbytes>=0.41.0

# =============================================================================
# 配置管理依赖 (必需)
# =============================================================================

# Python-dotenv - 环境变量管理
python-dotenv>=0.19.0

# =============================================================================
# 开发和测试工具 (可选)
# =============================================================================

# 测试框架
pytest>=7.0.0
pytest-cov>=4.0.0

# =============================================================================
# 说明
# =============================================================================

# 安装方式:
# pip install -r requirements-offline.txt

# 硬件建议:
# - GPU: 12GB+ VRAM (推荐RTX 3080/4070或更高)
# - RAM: 16GB+ 系统内存
# - 存储: 20GB+ 可用空间

# 安装顺序建议:
# 1. 首先安装PyTorch (CUDA版本)
# 2. 然后安装其他依赖

# CUDA PyTorch安装示例:
# pip install torch>=2.0.0 --index-url https://download.pytorch.org/whl/cu118

# 验证安装:
# python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}')"
# python -c "from transformers import AutoTokenizer; print('Transformers OK')"

# 注意事项:
# - 此配置确保完全离线运行
# - 不包含任何在线API依赖 (google-generativeai, openai等)
# - 支持FP8量化以提升推理速度
# - 所有推理都在本地设备上完成